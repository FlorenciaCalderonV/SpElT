{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b73f5f-7b90-40c5-92d4-f3569bd9c146",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load data from all sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c38af40-c0cb-4564-a210-3164dc8f28b9",
   "metadata": {},
   "source": [
    "#### Load name and path of all sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155729ab-ad2e-4fa4-a203-649520e7c883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from session_utils import find_all_sessions\n",
    "\n",
    "# Find all included sessions from Google sheet, with structure session_name: path\n",
    "session_dict = find_all_sessions(sheet_path = 'https://docs.google.com/spreadsheets/d/1_Xs5i-rHNTywV-WuQ8-TZliSjTxQCCqGWOD2AL_LIq0/edit#gid=0',\n",
    "                                 data_path = '/home/isabella/Documents/isabella/jake/recording_data',\n",
    "                                 sorting_suffix = 'sorting_ks2_custom')\n",
    "print(f'{len(session_dict.items())} sessions found')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87021594-d600-416a-9d37-7923efee2f4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Loop over all sessions, load data and filter for pyramidal cells\n",
    "### Criteria:\n",
    "#### - A. Cluster marked 'good' in phy\n",
    "#### - B. Cluster depth 0 +-200um\n",
    "#### - C. Mean firing rate between 0.1Hz and 10Hz\n",
    "#### - D. Template spike width >300us\n",
    "#### - E. Burst index - currently no cutoff value set\n",
    "#### - F. Spatial information - currently no cutoff value set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce5f843-a9a4-4eb1-8d0c-eba9cd178e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ephys import *\n",
    "from ephys_utils import select_spikes_by_trial, transform_spike_data, find_template_for_clusters\n",
    "from spatial_analysis import *\n",
    "\n",
    "for session, session_path in session_dict.items():\n",
    "    \n",
    "    # Create ephys object\n",
    "    obj = ephys(recording_type = 'nexus', path = session_path)\n",
    "    \n",
    "    ## A. Load good cells from phy\n",
    "    obj.load_spikes('good')\n",
    "    \n",
    "    # Get cluster info from phy\n",
    "    cluster_info = obj.spike_data['cluster_info']\n",
    "    # Get total good cells for session\n",
    "    total_cells = len(cluster_info.index)\n",
    "    \n",
    "    \n",
    "    ## B. Get cluster depths and exclude any outside of 0 +-200um\n",
    "    cluster_info = cluster_info[cluster_info['depth'].between(-200, 200)]\n",
    "    \n",
    "\n",
    "    ## C. Filter for mean firing rate between 1-10 Hz\n",
    "    cluster_info = cluster_info[cluster_info['fr'].between(0.1, 10)]\n",
    "\n",
    "    \n",
    "    ## D. Filter for spike width >300 us from template\n",
    "    if not cluster_info.empty:\n",
    "        # Find kilosort template for each cluster (closely approximates spike width) and add to dataframe\n",
    "        template_per_cluster = find_template_for_clusters(obj.spike_data['spike_clusters'], obj.spike_data['spike_templates'])\n",
    "        template_df = pd.DataFrame.from_dict(template_per_cluster, orient = 'index')\n",
    "        template_df.columns = ['template_id']\n",
    "\n",
    "        cluster_info = cluster_info.join(template_df, how = 'left')\n",
    "\n",
    "        # Load templates.npy\n",
    "        templates = np.load(f'{session_path}/{session_path[-8:-6]}{session_path[-5:-3]}{session_path[-2:]}_sorting_ks2_custom/templates.npy')\n",
    "        # Load inverse whitening matrix and apply to unwhiten templates\n",
    "        whitening_matrix_inv =  np.load(f'{session_path}/{session_path[-8:-6]}{session_path[-5:-3]}{session_path[-2:]}_sorting_ks2_custom/whitening_mat_inv.npy')\n",
    "        unwhitened_templates = np.einsum('ijk,kl->ijl', templates, whitening_matrix_inv)\n",
    "\n",
    "        # Add template values to dataframe\n",
    "        cluster_info['template'] = cluster_info.apply(lambda row: templates[int(row['template_id']), :, int(row['ch'])], axis=1)\n",
    "\n",
    "        # Work out template width peak to trough\n",
    "        sampling_rate = obj.spike_data['sampling_rate']\n",
    "\n",
    "        spike_width_samples = cluster_info['template'].apply(\n",
    "            lambda x: np.abs(np.argmax(x) - np.argmin(x))\n",
    "        )\n",
    "\n",
    "        # Convert spike width to microseconds and add to dataframe\n",
    "        spike_width_microseconds = (spike_width_samples/sampling_rate)*1000000\n",
    "        cluster_info['spike_width_microseconds'] = spike_width_microseconds\n",
    "\n",
    "        # Filter for spike width > 300us as in Wills et al., 2010\n",
    "        cluster_info = cluster_info[cluster_info['spike_width_microseconds'] > 300]\n",
    "        \n",
    "    ## E. Calculate burst index and filter\n",
    "    if not cluster_info.empty:\n",
    "        \n",
    "        # Reload spike data only for included cells\n",
    "        clusters_inc = list(cluster_info.index)\n",
    "        obj.load_spikes(clusters_to_load = clusters_inc)\n",
    "        \n",
    "        # Generate autocorrelograms and burst index for each cluster\n",
    "        from burst_index_and_autocorrelograms import *\n",
    "\n",
    "        spike_times_inc = obj.spike_data['spike_times']\n",
    "        spike_clusters_inc = obj.spike_data['spike_clusters']\n",
    "\n",
    "        autocorrelograms, first_moments = compute_autocorrelograms_and_first_moment(spike_times_inc, \n",
    "                                                                                     spike_clusters_inc, \n",
    "                                                                                     bin_size = 0.001, #1ms\n",
    "                                                                                     time_window = 0.05) #50ms\n",
    "        \n",
    "        cluster_info['first_moment_AC'] = first_moments.values()\n",
    "        \n",
    "        # Filter for first moment <25\n",
    "        cluster_info = cluster_info[cluster_info['first_moment_AC'] < 25]\n",
    "        \n",
    "#     ## F. Calculate spatial information and filter\n",
    "#     if not cluster_info.empty:\n",
    "#         # Load position data for all trials\n",
    "#         obj.load_pos([i for i, s in enumerate(obj.trial_list)], output_flag = False)\n",
    "        \n",
    "#         # Loop through trials and generate rate maps\n",
    "#         rate_maps = {}\n",
    "#         occupancy = {}\n",
    "\n",
    "#         for trial, trial_name in enumerate(obj.trial_list):\n",
    "\n",
    "#             # Select spikes for current trial and transform to create a dict of {cluster: spike_times, cluster:spike_times}\n",
    "#             current_trial_spikes = select_spikes_by_trial(obj.spike_data, trial, obj.trial_offsets)\n",
    "#             current_trial_spikes = transform_spike_data(current_trial_spikes)\n",
    "\n",
    "\n",
    "#             rate_maps[trial], occupancy[trial] = make_rate_maps(spike_data = current_trial_spikes,\n",
    "#                                        positions = obj.pos_data[trial]['xy_position'],  \n",
    "#                                        ppm = 400, \n",
    "#                                        x_bins = 50,\n",
    "#                                        y_bins = 50,\n",
    "#                                        dt = 1.0,\n",
    "#                                        smoothing_window = 10)\n",
    "            \n",
    "#         # Calculate spatial information - NEEDS ADDING\n",
    "        \n",
    "#         # Filter for spatial information > ??? - NEEDS ADDING\n",
    "        \n",
    "    \n",
    "    ## SAVE INCLUDED CLUSTER IDs TO .NPY\n",
    "    clusters_inc = cluster_info.index\n",
    "    n_clusters_inc = len(cluster_info.index)\n",
    "\n",
    "    np.save(f'{session_path}/clusters_inc.npy', clusters_inc)        \n",
    "        \n",
    "    print(f'Session {session}: {n_clusters_inc} cells retained of {total_cells} good cells from phy. Retained cells: {clusters_inc.values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6efb97-d0ca-4f75-9612-5d80e62daf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spike_clusters_inc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a9d9e5-f757-45ca-9d0a-b5b0dfb86c35",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 7. Calculate spatial information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db901528-7edc-46e2-8c05-1c819bf2044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def calculate_spatial_information(rate_maps, occupancy, dt=1.0):\n",
    "    \"\"\"\n",
    "    Calculate Skaggs' spatial information score for given rate maps and occupancy.\n",
    "    \n",
    "    Parameters:\n",
    "    - rate_maps: dict\n",
    "        Dictionary containing smoothed rate maps organized by clusters.\n",
    "    - occupancy: np.ndarray\n",
    "        2D array indicating occupancy of each bin.\n",
    "    - dt: float\n",
    "        Time window for spike count.\n",
    "        \n",
    "    Returns:\n",
    "    - skaggs_info_dict: dict\n",
    "        Dictionary containing Skaggs' spatial information scores organized by clusters.\n",
    "    \"\"\"\n",
    "    \n",
    "    skaggs_info_dict = {}\n",
    "    \n",
    "    # Calculate the total time spent in the environment\n",
    "    total_time = np.nansum(occupancy) * dt\n",
    "\n",
    "    for cluster, rate_map in rate_maps.items():\n",
    "        \n",
    "        # Calculate mean firing rate across all bins\n",
    "        mean_firing_rate = np.nansum(rate_map * occupancy) / total_time\n",
    "\n",
    "        # Calculate probability of occupancy for each bin\n",
    "        prob_occupancy = occupancy / np.nansum(occupancy)\n",
    "\n",
    "        # Calculate Skaggs' spatial information score\n",
    "        non_zero_idx = (rate_map > 0) & (prob_occupancy > 0)\n",
    "        skaggs_info = np.nansum(\n",
    "            prob_occupancy[non_zero_idx] *\n",
    "            rate_map[non_zero_idx] *\n",
    "            np.log2(rate_map[non_zero_idx] / mean_firing_rate)\n",
    "        )\n",
    "\n",
    "        skaggs_info_dict[cluster] = skaggs_info\n",
    "            \n",
    "    return skaggs_info_dict\n",
    "\n",
    "# Calculate spatial information - NEEDS ADJUSTING TO HANDLE NAN VALUES\n",
    "spatial_info = {}\n",
    "\n",
    "for i in rate_maps.keys():\n",
    "    spatial_info[i] = calculate_spatial_information(rate_maps[i], occupancy[i], dt = 1)\n",
    "spatial_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de2392-c31b-47ad-9b2a-017d6257243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_info"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_37",
   "language": "python",
   "name": "env_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
