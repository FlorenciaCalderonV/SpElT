{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7705120-8596-455c-b10f-d6a21a94c0ca",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Load data for all pyramidal cells from all trials into a dataframe of ephys objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e2fd9d-c69e-43e1-9445-9d0e82f49b6c",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from session_utils import *\n",
    "from ephys import *\n",
    "from ephys_utils import select_spikes_by_trial\n",
    "\n",
    "lfp_sampling_rate = 1000\n",
    "\n",
    "# Load all session names and paths as dict\n",
    "session_list = find_all_sessions(sheet_path = 'https://docs.google.com/spreadsheets/d/1_Xs5i-rHNTywV-WuQ8-TZliSjTxQCCqGWOD2AL_LIq0/edit#gid=0',\n",
    "                                 data_path = '/home/isabella/Documents/isabella/jake/recording_data',\n",
    "                                 sorting_suffix = 'sorting_ks2_custom')\n",
    "\n",
    "# Initialise DataFrame and explicitly set dtype for 'clusters_inc' to 'object'\n",
    "df_all_cells = pd.DataFrame(data = None, index = session_list.keys(), columns=['clusters_inc'], dtype='object')\n",
    "\n",
    "\n",
    "for i, session_path in enumerate(session_list.values()):\n",
    "    session = list(session_list.keys())[i]\n",
    "    \n",
    "    # Get IDs of included clusters from postprocessing.Select Clusters All Sessions.ipynb\n",
    "    clusters_inc = np.load(f'{session_path}/clusters_inc.npy', allow_pickle = True)\n",
    "    \n",
    "    if len(clusters_inc) > 0:\n",
    "        df_all_cells.at[session, 'clusters_inc'] = clusters_inc\n",
    "\n",
    "        # Create ephys object for session and add to dataframe\n",
    "        obj = ephys(recording_type = 'nexus', path = session_path)\n",
    "\n",
    "        # Load spikes for all included clusters\n",
    "        obj.load_spikes(clusters_inc)\n",
    "        \n",
    "        # Reorganise spikes by trial, starting at t=0 at each trial\n",
    "        obj.spikes_by_trial = select_spikes_by_trial(obj.spike_data, obj.trial_iterators, obj.trial_offsets)\n",
    "\n",
    "        # Load position data for all trials\n",
    "        obj.load_pos(obj.trial_iterators, output_flag = False, reload_flag = False)\n",
    "\n",
    "        # Get unique channels with included cells\n",
    "        obj.good_channels = np.unique(obj.spike_data['cluster_info'].loc(axis=1)['ch'])\n",
    "        \n",
    "        # # Load LFP for all trials for channels with units\n",
    "        # obj.load_lfp(obj.trial_iterators, lfp_sampling_rate, channels = obj.good_channels, reload_flag = True)\n",
    "\n",
    "        # Add data to frame\n",
    "        df_all_cells.at[list(session_list.keys())[i], 'ephys_object'] = obj\n",
    "\n",
    "    print(f'Loaded session {session}')\n",
    "\n",
    "# Pickle dataframe for loading elsewhere\n",
    "df_all_cells.to_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells.pkl')\n",
    "print(f'All data saved to /home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells.pkl')\n",
    "\n",
    "df_all_cells.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d189f8-c41f-4416-8da2-25d81a0189b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from position_analysis import *\n",
    "from get_theta_frequencies import get_theta_frequencies\n",
    "from get_theta_phase import get_spike_theta_phase\n",
    "from ephys import *\n",
    "from postprocessing.spatial_analysis import speed_filter_spikes\n",
    "from rayleigh_vector import rayleigh_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a6e6e-5eb9-44b2-9c6a-a07d7e9ac850",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate theta phase for all pyramidal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb507b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "df_all_cells = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells.pkl')\n",
    "\n",
    "lfp_sampling_rate = 1000\n",
    "\n",
    "# Drop rows with no included clusters\n",
    "df_all_cells = df_all_cells.dropna()\n",
    "print(f'{len(df_all_cells.index)} sessions retained')\n",
    "\n",
    "# Loop through all sessions\n",
    "for index, obj in df_all_cells['ephys_object'].items():\n",
    "    \n",
    "    # # Find t-maze and open field trials for session\n",
    "    # t_maze_trials = [i for i, s in enumerate(obj.trial_list) if 't-maze' in s]\n",
    "    # open_field_trials = [i for i, s in enumerate(obj.trial_list) if 'open-field' in s]\n",
    "    \n",
    "    # Initialise output variables\n",
    "    obj.cluster_phases = [{} for _ in range(len(obj.trial_list))]\n",
    "    obj.cluster_sectors = [{} for _ in range(len(obj.trial_list))]\n",
    "    obj.cluster_rayleigh = [{} for _ in range(len(obj.trial_list))]\n",
    "    \n",
    "    # Load LFP if not already loaded\n",
    "    obj.load_lfp(obj.trial_iterators, lfp_sampling_rate, channels = obj.good_channels, reload_flag = False)\n",
    "\n",
    "    # Load LFP and position for each channel, get theta phase and position sector for each spike\n",
    "    for i in obj.trial_iterators:\n",
    "        \n",
    "        # Extract XY position data for trial\n",
    "        trial_pos = obj.pos_data[i]['xy_position']\n",
    "\n",
    "        # Convert to DataFrame and rename columns to match original channel\n",
    "        lfp_df = pd.DataFrame(obj.lfp_data[i]['data'])\n",
    "        lfp_df.columns = obj.good_channels\n",
    "\n",
    "        # Find peak theta frequencies for each channel and make dict of {channel: theta_freq}\n",
    "        theta_freqs = get_theta_frequencies(obj.lfp_data[i]['data'], lfp_sampling_rate)\n",
    "        theta_freqs = dict(zip(obj.good_channels, theta_freqs))\n",
    "\n",
    "\n",
    "        # Loop through each good cluster\n",
    "        for cluster in obj.spike_data['cluster_info'].index:\n",
    "            # Get channel for cluster\n",
    "            channel = obj.spike_data['cluster_info'].loc[cluster, 'ch']\n",
    "\n",
    "            # Get peak theta frequency\n",
    "            theta_freq = theta_freqs[channel]\n",
    "\n",
    "            ## SPIKE PHASE\n",
    "            # Extract LFP data for the recording channel\n",
    "            lfp_data_for_channel = lfp_df.loc[:, channel]\n",
    "\n",
    "            # Extract spike times for the cluster\n",
    "            cluster_spike_times = obj.spikes_by_trial[i]['spike_times'][obj.spikes_by_trial[i]['spike_clusters'] == cluster]\n",
    "            \n",
    "            # Filter for speed (function from spatial_analysis.py)\n",
    "            spike_times_filtered = speed_filter_spikes(current_trial_spikes = {cluster: cluster_spike_times}, #Needs to be dict, key not important\n",
    "                                                        speed_data = obj.pos_data[i]['speed'],\n",
    "                                                        position_sampling_rate = obj.pos_data[i]['pos_sampling_rate'],\n",
    "                                                        speed_lower_bound = 10, #2.5 cm/s\n",
    "                                                        speed_upper_bound = 400) #100 cm/s\n",
    "\n",
    "            # Compute theta phases for the spike times\n",
    "            spike_phases = get_spike_theta_phase(lfp_data_for_channel, spike_times_filtered[cluster], lfp_sampling_rate, theta_freq)\n",
    "\n",
    "            # Store in the dictionary\n",
    "            obj.cluster_phases[i][cluster] = spike_phases    \n",
    "\n",
    "            ## SPIKE POSITION SECTOR\n",
    "            # Assign spike times to nearest position sector\n",
    "            spike_pos = [[], []]\n",
    "            for t in spike_times_filtered[cluster]:\n",
    "                spike_pos_index = round(t * 50)/50\n",
    "                spike_pos[0].append(trial_pos[spike_pos_index].values[0])\n",
    "                spike_pos[1].append(trial_pos[spike_pos_index].values[1])\n",
    "\n",
    "            # spike_pos is now an array with the structure  [[x1, x2, x3 ...], [y1, y2, y3 ...]] for each spike\n",
    "            # Get position sector for each spike\n",
    "            spike_sectors = assign_sectors(pd.DataFrame(spike_pos), pos_header = obj.pos_data[i]['header'])\n",
    "\n",
    "            # add sectors to dict\n",
    "            obj.cluster_sectors[i][cluster] = spike_sectors\n",
    "\n",
    "            ## RAYLEIGH VECTOR\n",
    "            # Compute rayleigh vector for each cluster\n",
    "            obj.cluster_rayleigh[i][cluster] = rayleigh_vector(spike_phases)\n",
    "    \n",
    "    \n",
    "    # Re-assign object to dataframe\n",
    "    df_all_cells.loc[index, 'ephys_object'] = obj\n",
    "    print(f'Session {index} processed')\n",
    "\n",
    "# Pickle dataframe for loading elsewhere\n",
    "df_all_cells.to_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells_theta.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215874fc",
   "metadata": {},
   "source": [
    "## Plot theta phase vs position sector for t-maze and theta phase for open field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34260dfe-bb60-45a8-85ac-57fd61a441a6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plotting functions\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "# Function to get phases for a given sector within a specific cluster\n",
    "def get_phases_for_sector_in_cluster(cluster_sectors, cluster_phases, sector, cluster_id):\n",
    "    indices = np.where(cluster_sectors[int(cluster_id)] == sector)[0]\n",
    "    return cluster_phases[int(cluster_id)][indices]\n",
    "\n",
    "# Function to plot polar plots for a specific cluster\n",
    "def plot_theta_phase_by_position_cluster(obj, trial_iterator, cluster_id, session):\n",
    "    \n",
    "    # Define the directory where you want to save the figure\n",
    "    save_directory = f'/home/isabella/Documents/isabella/jake/ephys_analysis/figures/P{obj.age}_{session}'\n",
    "    \n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "        \n",
    "    ### For T-MAZE TRIALS:\n",
    "    if 't-maze' in obj.trial_list[trial_iterator]:\n",
    "        unique_sectors = range(1,13)\n",
    "        fig, axes = plt.subplots(nrows=3, ncols=4, figsize=(15, 10), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "        for idx, sector in enumerate(unique_sectors):\n",
    "            row_idx = idx // 4\n",
    "            col_idx = idx % 4\n",
    "            sector = int(sector)\n",
    "\n",
    "            phases = get_phases_for_sector_in_cluster(obj.cluster_sectors[trial_iterator], obj.cluster_phases[trial_iterator], sector, cluster_id)\n",
    "\n",
    "            # Remove any NaN/ infinite values\n",
    "            phases = phases[np.isfinite(phases)]\n",
    "\n",
    "            # Calculate circular mean of theta phase\n",
    "            mean_phase = scipy.stats.circmean(phases, nan_policy = 'omit')\n",
    "            std_phase = scipy.stats.circstd(phases, nan_policy = 'omit')\n",
    "\n",
    "            # Histogram of the phases\n",
    "            n, bins, patches = axes[row_idx, col_idx].hist(phases, bins=30, alpha=0.6)\n",
    "\n",
    "            # Add a red line indicating the mean phase\n",
    "            axes[row_idx, col_idx].axvline(mean_phase, color='r', linestyle='--', linewidth=2)\n",
    "\n",
    "            # Add a sector indicating standard deviation\n",
    "            # axes[row_idx, col_idx].axvspan(mean_phase-std_phase, mean_phase+std_phase, color='r', alpha = 0.4)\n",
    "\n",
    "            # Set the title for the current subplot\n",
    "            axes[row_idx, col_idx].set_title(f\"Sector {sector}\")\n",
    "\n",
    "        plt.suptitle(f'Theta phase vs position sector for cluster {cluster_id} from trial {obj.trial_list[trial_iterator]}', fontsize = 25)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_directory}/{cluster_id}_trial-{trial_iterator}_theta_phase_by_sector.png')\n",
    "        plt.show()\n",
    "        \n",
    "    ### FOR OPEN FIELD TRIALS\n",
    "    if 'open-field' in obj.trial_list[trial_iterator]:\n",
    "        \n",
    "        # Get all phases for cluster\n",
    "        phases = obj.cluster_phases[trial_iterator][cluster_id]\n",
    "        \n",
    "        # Remove any NaN/ infinite values\n",
    "        phases = phases[np.isfinite(phases)]   \n",
    "        \n",
    "        # Calculate circular mean of theta phase\n",
    "        mean_phase = scipy.stats.circmean(phases, nan_policy = 'omit')\n",
    "        std_phase = scipy.stats.circstd(phases, nan_policy = 'omit')\n",
    "        \n",
    "        # Create plot\n",
    "        fig, axes = plt.subplots(figsize = (5, 5), subplot_kw={'projection': 'polar'})\n",
    "        \n",
    "        # Histogram of the phases\n",
    "        n, bins, patches = axes.hist(phases, bins=30, alpha=0.6)\n",
    "\n",
    "        # Add a red line indicating the mean phase\n",
    "        axes.axvline(mean_phase, color='r', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.suptitle(f'Theta phase distribution for cluster {cluster_id}\\n from trial {obj.trial_list[trial_iterator]}', fontsize = 15)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_directory}/{cluster_id}_trial-{trial_iterator}_theta_phase_by_sector.png')\n",
    "        plt.show()\n",
    "    \n",
    "#### Plot theta phase vs position\n",
    "\n",
    "# Loop through all sessions and make all plots per cluster\n",
    "for session, obj in df_all_cells['ephys_object'].items():\n",
    "    \n",
    "    # Loop through each good cluster\n",
    "    for cluster in obj.spike_data['cluster_info'].index:\n",
    "        \n",
    "        # Loop through each trial and plot 12-sector plot for t-maze single polar plot for open field\n",
    "        for trial in obj.trial_iterators:\n",
    "            plot_theta_phase_by_position_cluster(obj, trial, cluster, session)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac944b4a-544b-46a9-ae2d-c1a8e547065e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot rate maps for pyramidal cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a3656a-ac41-4963-b6e7-ed94d80a0a22",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from postprocessing.spatial_analysis import make_rate_maps, plot_cluster_across_sessions, speed_filter_spikes\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ephys_utils import select_spikes_by_trial, transform_spike_data\n",
    "from spatial_information import spatial_info\n",
    "\n",
    "\n",
    "# Load pickled data\n",
    "df_all_cells = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells.pkl') if df_all_cells is None else df_all_cells\n",
    "# Drop rows with no included clusters\n",
    "df_all_cells = df_all_cells.dropna()\n",
    "print(f'{len(df_all_cells.index)} sessions retained')\n",
    "\n",
    "spatial_info_all = pd.DataFrame(columns = ['bits_per_spike', 'bits_per_sec', 'age'], dtype = 'object')\n",
    "rates_all = pd.DataFrame(columns = ['mean_rates', 'max_rates', 'age'], dtype = 'object')\n",
    "rate_maps_all = pd.DataFrame(columns = ['open-field_1', 'open-field_2', 't-maze_1', 't-maze_2', 'age'], dtype = 'object')\n",
    "occupancy_maps_all = pd.DataFrame(columns = ['open-field_1', 'open-field_2', 't-maze_1', 't-maze_2', 'age'], dtype = 'object')\n",
    "\n",
    "# Calculate rate maps\n",
    "for session, obj in df_all_cells['ephys_object'].items():\n",
    "    \n",
    "    # Loop through trials and generate rate maps\n",
    "    rate_maps = {}\n",
    "    raw_rate_maps = {}\n",
    "    smoothed_pos_maps = {}\n",
    "    raw_pos_maps = {}\n",
    "    max_rates = {}\n",
    "    mean_rates = {}\n",
    "    bits_per_spike = {}\n",
    "    bits_per_sec = {}\n",
    "    \n",
    "    # Make rate maps for all trials\n",
    "    for trial, trial_name in enumerate(obj.trial_list):\n",
    "        \n",
    "#         # Load unloaded position data if any\n",
    "#         obj.load_pos(trial, reload_flag = False)\n",
    "\n",
    "        # Select spikes for current trial and transform to create a dict of {cluster: spike_times}\n",
    "        current_trial_spikes = select_spikes_by_trial(obj.spike_data, trial, obj.trial_offsets)\n",
    "        current_trial_spikes = transform_spike_data(current_trial_spikes)[trial]\n",
    "        \n",
    "        # Filter spikes for speed\n",
    "        current_trial_spikes_filtered = speed_filter_spikes(current_trial_spikes,\n",
    "                                                            speed_data = obj.pos_data[trial]['speed'],\n",
    "                                                            position_sampling_rate = obj.pos_data[trial]['pos_sampling_rate'],\n",
    "                                                            speed_lower_bound = 2.5, #2.5 cm/s\n",
    "                                                            speed_upper_bound = 100) #100 cm/s\n",
    "        \n",
    "        # Calculate rate maps\n",
    "        rate_maps[trial], raw_rate_maps[trial], smoothed_pos_maps[trial], raw_pos_maps[trial], max_rates[trial], mean_rates[trial] = make_rate_maps(spike_data = current_trial_spikes_filtered,\n",
    "                                   pos_data = obj.pos_data[trial], \n",
    "                                   adaptive_smoothing = True,\n",
    "                                   alpha = 200) # for adaptive smoothing\n",
    "\n",
    "\n",
    "        \n",
    "        # Calculate spatial information from UNSMOOTHED rate and pos maps\n",
    "        bits_per_spike[trial], bits_per_sec[trial] = spatial_info(raw_rate_maps[trial], raw_pos_maps[trial])\n",
    "        \n",
    "    \n",
    "        # Save rate maps & spatial information to dfs\n",
    "        for cluster in rate_maps[trial].keys():\n",
    "\n",
    "            spatial_info_all.loc[f'{trial_name}_{cluster}', ['bits_per_spike', 'bits_per_sec', 'age']] = [bits_per_spike[trial][cluster], bits_per_sec[trial][cluster], obj.age]\n",
    "            rates_all.loc[f'{trial_name}_{cluster}', ['mean_rates', 'max_rates', 'age']] = [mean_rates[trial][cluster], max_rates[trial][cluster], obj.age]\n",
    "\n",
    "            trial_type = f'{trial_name.split(\"_\")[-2]}_{trial_name.split(\"_\")[-1]}'\n",
    "            rate_maps_all.at[f'{trial_name.split(\"_\")[0]}_{trial_name.split(\"_\")[1]}_{cluster}', trial_type] = rate_maps[trial][cluster]\n",
    "            rate_maps_all.at[f'{trial_name.split(\"_\")[0]}_{trial_name.split(\"_\")[1]}_{cluster}', 'age'] = obj.age\n",
    "\n",
    "        occupancy_maps_all.at[f'{trial_name.split(\"_\")[0]}_{trial_name.split(\"_\")[1]}', trial_type] = smoothed_pos_maps[trial]\n",
    "        occupancy_maps_all.at[f'{trial_name.split(\"_\")[0]}_{trial_name.split(\"_\")[1]}', 'age'] = obj.age\n",
    "\n",
    "\n",
    "\n",
    "    ## RATE MAP PLOTTING\n",
    "\n",
    "    # Define the directory where you want to save the figures\n",
    "    save_directory = f'/home/isabella/Documents/isabella/jake/ephys_analysis/figures/P{obj.age}_{session}'\n",
    "    \n",
    "    # Check if the directory exists, if not, create it\n",
    "    if not os.path.exists(save_directory):\n",
    "        os.makedirs(save_directory)\n",
    "    \n",
    "    # Plot rate maps for all clusters in session\n",
    "    for cluster in obj.spike_data['cluster_info'].index:\n",
    "        plot_cluster_across_sessions(rate_maps_dict = rate_maps,\n",
    "                                     cluster_id = cluster,\n",
    "                                     max_rates_dict =  max_rates,\n",
    "                                     mean_rates_dict = mean_rates,\n",
    "                                     spatial_info_dict = bits_per_spike,\n",
    "                                     session = session,\n",
    "                                     age = obj.age)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_directory}/{cluster}_rate_maps.png')\n",
    "        plt.show()\n",
    "            \n",
    "    # Save data to pickle\n",
    "    rate_maps_all.to_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/rate_maps_all.pkl')\n",
    "    occupancy_maps_all.to_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/occupancy_maps_all.pkl')\n",
    "    spatial_info_all.to_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/spatial_info_all.pkl')\n",
    "    rates_all.to_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/rates_all.pkl')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d0397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.io import savemat\n",
    "\n",
    "# rate_maps_open_field_1 = rate_maps_all.to_dict()['open-field_1']\n",
    "# rate_maps_open_field_2 = rate_maps_all.to_dict()['open-field_2']\n",
    "# rate_maps_t_maze_1 = rate_maps_all.to_dict()['t-maze_1']\n",
    "# rate_maps_t_maze_2 = rate_maps_all.to_dict()['t-maze_2']\n",
    "# occupancy_maps_open_field_1 = occupancy_maps_all.to_dict()['open-field_1']\n",
    "# occupancy_maps_open_field_2 = occupancy_maps_all.to_dict()['open-field_2']\n",
    "# occupancy_maps_t_maze_1 = occupancy_maps_all.to_dict()['t-maze_1']\n",
    "# occupancy_maps_t_maze_2 = occupancy_maps_all.to_dict()['t-maze_2']\n",
    "\n",
    "# maps_list = [rate_maps_open_field_1, rate_maps_open_field_2, rate_maps_t_maze_1, rate_maps_t_maze_2,\n",
    "#              occupancy_maps_open_field_1, occupancy_maps_open_field_2, occupancy_maps_t_maze_1, occupancy_maps_t_maze_2]\n",
    "\n",
    "# # add letter to each value in dict keys to make them valid matlab field names\n",
    "# for m in maps_list[0:4]:\n",
    "#     keys = list(m.keys())\n",
    "#     for k in keys:\n",
    "#         m['a' + k] = m.pop(k)\n",
    "# for m in maps_list[4:8]:\n",
    "#     keys = list(m.keys())\n",
    "#     for k in keys:\n",
    "#         m['b' + k] = m.pop(k)\n",
    "\n",
    "# savemat('rate_maps_open-field_1.mat', maps_list[0])\n",
    "# savemat('rate_maps_open-field_2.mat', maps_list[1])\n",
    "# savemat('rate_maps_t-maze_1.mat', maps_list[2])\n",
    "# savemat('rate_maps_t-maze_2.mat', maps_list[3])\n",
    "# savemat('occupancy_maps_open-field_1.mat', maps_list[4])\n",
    "# savemat('occupancy_maps_open-field_2.mat', maps_list[5])\n",
    "# savemat('occupancy_maps_t-maze_1.mat', maps_list[6])\n",
    "# savemat('occupancy_maps_t-maze_2.mat', maps_list[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1678c099",
   "metadata": {},
   "source": [
    "## Spatial Information & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39ba726",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Set age bins\n",
    "age_bins = [(22, 26), (27, 29), (30, 35)]\n",
    "\n",
    "# Load pickled rate map data\n",
    "rate_maps_all = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/rate_maps_all.pkl')\n",
    "spatial_info_df = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/spatial_info_all.pkl')\n",
    "\n",
    "# Format spatial info dataframe\n",
    "spatial_info_df.dropna(inplace = True)\n",
    "spatial_info_df.reset_index(inplace = True)\n",
    "spatial_info_df = spatial_info_df.rename(columns = {'index': 'cell_id'})\n",
    "spatial_info_df['trial_type'] = spatial_info_df['cell_id'].apply(lambda x: x.split('_')[-3])\n",
    "spatial_info_df['age'] = spatial_info_df['age'].astype(int)\n",
    "\n",
    "# Group by age bin\n",
    "spatial_info_df['age_bin'] = pd.cut(spatial_info_df['age'], bins = [22, 26, 29, 35], labels = ['22-26', '27-29', '30-35'])\n",
    "# Convert 'age_bin' column to categorical data type\n",
    "spatial_info_df['age_bin'] = pd.Categorical(spatial_info_df['age_bin'], categories=['22-26', '27-29', '30-35'], ordered=True)\n",
    "\n",
    "# Calculate mean and std of bits_per_spike and bits_per_sec\n",
    "spatial_info_df_stats = spatial_info_df.groupby('age_bin').agg({'bits_per_spike': ['mean', 'std'], 'bits_per_sec': ['mean', 'std']}).reset_index()\n",
    "\n",
    "# Perform ANOVA on age bin and bits per spike\n",
    "anova_bits_per_spike = stats.f_oneway(spatial_info_df[spatial_info_df['age_bin'] == '22-26']['bits_per_spike'],\n",
    "                       spatial_info_df[spatial_info_df['age_bin'] == '27-29']['bits_per_spike'],\n",
    "                       spatial_info_df[spatial_info_df['age_bin'] == '30-35']['bits_per_spike'])\n",
    "print(f'ANOVA for bits per spike: {anova_bits_per_spike}')\n",
    "\n",
    "# Perform ANOVA on age bin and bits per sec\n",
    "anova_bits_per_sec = stats.f_oneway(spatial_info_df[spatial_info_df['age_bin'] == '22-26']['bits_per_sec'],\n",
    "                       spatial_info_df[spatial_info_df['age_bin'] == '27-29']['bits_per_sec'],\n",
    "                       spatial_info_df[spatial_info_df['age_bin'] == '30-35']['bits_per_sec'])\n",
    "print(f'ANOVA for bits per sec: {anova_bits_per_sec}')\n",
    "\n",
    "# Replace string age bins with integers\n",
    "age_bin_mapping = {'22-26': 1, '27-29': 2, '30-35': 3}\n",
    "spatial_info_df['age_bin'] = spatial_info_df['age_bin'].replace(age_bin_mapping)\n",
    "\n",
    "if anova_bits_per_spike[1] < 0.05:\n",
    "    # Perform Tukey HSD test for bits per spike\n",
    "    tukey_bits_per_spike = pairwise_tukeyhsd(spatial_info_df['bits_per_spike'].astype(float), spatial_info_df['age_bin'])\n",
    "    print(f'Tukey HSD for bits per spike: {tukey_bits_per_spike}')\n",
    "else:\n",
    "    print('ANOVA for bits per spike not significant, skipping Tukey HSD test')\n",
    "\n",
    "# if p < 0.05, perform Tukey HSD test\n",
    "if anova_bits_per_sec[1] < 0.05:\n",
    "    # Perform Tukey HSD test for bits per sec\n",
    "    tukey_bits_per_sec = pairwise_tukeyhsd(spatial_info_df['bits_per_sec'].astype(float), spatial_info_df['age_bin'])\n",
    "    print(f'Tukey HSD for bits per sec: {tukey_bits_per_sec}')\n",
    "else:\n",
    "    print('ANOVA for bits per sec not significant, skipping Tukey HSD test')\n",
    "\n",
    "# Plot bits_per_spike and bits_per_sec for each age bin\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))\n",
    "axes[0].errorbar(x = spatial_info_df_stats['age_bin'], y = spatial_info_df_stats['bits_per_spike']['mean'], yerr = spatial_info_df_stats['bits_per_spike']['std'], fmt = 'o')\n",
    "axes[0].set_title('Bits per spike')\n",
    "axes[0].set_xlabel('Age bin')\n",
    "axes[0].set_ylabel('Bits per spike')\n",
    "axes[0].set_ylim(0, 1.5)\n",
    "\n",
    "axes[1].errorbar(x = spatial_info_df_stats['age_bin'], y = spatial_info_df_stats['bits_per_sec']['mean'], yerr = spatial_info_df_stats['bits_per_sec']['std'], fmt = 'o')\n",
    "axes[1].set_title('Bits per second')\n",
    "axes[1].set_xlabel('Age bin')\n",
    "axes[1].set_ylabel('Bits per second')\n",
    "axes[1].set_ylim(0, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29817272",
   "metadata": {},
   "source": [
    "## Spatial Correlation & Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600df63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from spatial_correlation import spatial_correlation\n",
    "from scipy import stats\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "age_bins = [(22, 26), (27, 29), (30, 35)]\n",
    "\n",
    "# Load pickled rate map data\n",
    "rate_maps_all = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/rate_maps_all.pkl')\n",
    "\n",
    "\n",
    "# Calculate spatial correlation for each cluster across both trial types\n",
    "for trial_type in ['open-field', 't-maze']:\n",
    "    rate_maps_correlation = rate_maps_all[[f'{trial_type}_1', f'{trial_type}_2']].dropna()\n",
    "\n",
    "    spatial_correlations = spatial_correlation(rate_maps_correlation[f'{trial_type}_1'].to_list(), rate_maps_correlation[f'{trial_type}_2'].to_list())\n",
    "\n",
    "    rate_maps_all.loc[rate_maps_correlation.index, f'spatial_correlation_{trial_type}'] = spatial_correlations\n",
    "\n",
    "rate_maps_all\n",
    "\n",
    "# Keep only age and spatial correlation columns\n",
    "spatial_correlations = rate_maps_all[['age', 'spatial_correlation_open-field', 'spatial_correlation_t-maze']].dropna().copy()\n",
    "# Convert age to int\n",
    "spatial_correlations['age'] = spatial_correlations['age'].astype(int)\n",
    "\n",
    "# Group by age bin\n",
    "spatial_correlations['age_bin'] = pd.cut(spatial_correlations['age'], bins = [22, 26, 29, 35], labels = ['22-26', '27-29', '30-35'])\n",
    "# Convert 'age_bin' column to categorical data type\n",
    "spatial_correlations['age_bin'] = pd.Categorical(spatial_correlations['age_bin'], categories=['22-26', '27-29', '30-35'], ordered=True)\n",
    "\n",
    "# Calculate mean and std of spatial info correlations\n",
    "spatial_correlations_stats = spatial_correlations.groupby('age_bin').agg({'spatial_correlation_open-field': ['mean', 'std'], 'spatial_correlation_t-maze': ['mean', 'std']}).reset_index()\n",
    "\n",
    "# Perform ANOVA on age bin and open field spatial correlation\n",
    "anova_open_field = stats.f_oneway(spatial_correlations[spatial_correlations['age_bin'] == '22-26']['spatial_correlation_open-field'],\n",
    "                       spatial_correlations[spatial_correlations['age_bin'] == '27-29']['spatial_correlation_open-field'],\n",
    "                       spatial_correlations[spatial_correlations['age_bin'] == '30-35']['spatial_correlation_open-field'])\n",
    "print(f'ANOVA for spatial_correlation_open-field: {anova_open_field}')\n",
    "\n",
    "# Perform ANOVA on age bin and t-maze spatial correlation\n",
    "anova_tmaze = stats.f_oneway(spatial_correlations[spatial_correlations['age_bin'] == '22-26']['spatial_correlation_t-maze'],\n",
    "                       spatial_correlations[spatial_correlations['age_bin'] == '27-29']['spatial_correlation_t-maze'],\n",
    "                       spatial_correlations[spatial_correlations['age_bin'] == '30-35']['spatial_correlation_t-maze'])\n",
    "print(f'ANOVA for spatial_correlation_t-maze: {anova_tmaze}')\n",
    "\n",
    "# Replace string age bins with integers\n",
    "age_bin_mapping = {'22-26': 1, '27-29': 2, '30-35': 3}\n",
    "spatial_correlations['age_bin'] = spatial_correlations['age_bin'].replace(age_bin_mapping)\n",
    "\n",
    "# if p < 0.05, perform Tukey HSD test\n",
    "if anova_open_field[1] < 0.05:\n",
    "    print('ANOVA significant, performing Tukey HSD test')\n",
    "    # Perform Tukey HSD test for open field spatial correlation\n",
    "    tukey_open_field = pairwise_tukeyhsd(spatial_correlations['spatial_correlation_open-field'], spatial_correlations['age_bin'])\n",
    "    print(f'Tukey HSD for spatial_correlation_open-field: {tukey_open_field}')\n",
    "else:\n",
    "    print('ANOVA for open field not significant, skipping Tukey HSD test')\n",
    "\n",
    "if anova_tmaze[1] < 0.05:\n",
    "    # Perform Tukey HSD test for t-maze spatial correlation\n",
    "    tukey_tmaze = pairwise_tukeyhsd(spatial_correlations['spatial_correlation_t-maze'], spatial_correlations['age_bin'])\n",
    "    print(f'Tukey HSD for bits spatial_correlation_t-maze: {tukey_tmaze}')\n",
    "else:\n",
    "    print('ANOVA for T-maze not significant, skipping Tukey HSD test')\n",
    "\n",
    "# Plot spatial correlation for open field and t-maze for each age bin\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (10, 5))\n",
    "axes[0].errorbar(x = spatial_correlations_stats['age_bin'], y = spatial_correlations_stats['spatial_correlation_open-field']['mean'], yerr = spatial_correlations_stats['spatial_correlation_open-field']['std'], fmt = 'o')\n",
    "axes[0].set_title('Spatial Correlation Open Field')\n",
    "axes[0].set_xlabel('Age bin')\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].set_ylabel('Spatial Correlation')\n",
    "\n",
    "axes[1].errorbar(x = spatial_correlations_stats['age_bin'], y = spatial_correlations_stats['spatial_correlation_t-maze']['mean'], yerr = spatial_correlations_stats['spatial_correlation_t-maze']['std'], fmt = 'o')\n",
    "axes[1].set_title('Spatial Correlation T-Maze')\n",
    "axes[1].set_xlabel('Age bin')\n",
    "axes[1].set_ylabel('Spatial Correlation')\n",
    "axes[1].set_ylim(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff048a1f-701f-459d-ae67-922b17e55d86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plot autocorrelograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab316543",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add spatial info to mean_rates_all\n",
    "mean_rates_all = pd.DataFrame.from_dict(rates_all, orient='index')\n",
    "mean_rates_all = mean_rates_all.reset_index()\n",
    "\n",
    "mean_rates_all['bits_per_sec'] = mean_rates_all['index'].apply(lambda x: spatial_info_all[x]['bits_per_sec'])\n",
    "mean_rates_all['bits_per_spike'] = mean_rates_all['index'].apply(lambda x: spatial_info_all[x]['bits_per_spike'])\n",
    "\n",
    "\n",
    "# Plot mean rate against bits_per_sec\n",
    "fig, ax = plt.subplots(figsize = (10, 10))\n",
    "ax.scatter(x = mean_rates_all['mean_rates'], y = mean_rates_all['bits_per_sec'])\n",
    "ax.set_xlabel('Mean rate (Hz)')\n",
    "ax.set_ylabel('Bits per second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72e0c40-486d-4bb0-abc9-217a61880fe0",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from postprocessing.burst_index_and_autocorrelograms import compute_autocorrelograms_and_first_moment, plot_autocorrelogram\n",
    "\n",
    "# Load pickled data\n",
    "df_all_cells = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells.pkl')\n",
    "# Drop rows with no included clusters\n",
    "df_all_cells = df_all_cells.dropna()\n",
    "print(f'{len(df_all_cells.index)} sessions retained')\n",
    "\n",
    "# Loop through all sessions and make all plots per cluster\n",
    "for session, obj in df_all_cells['ephys_object'].items():\n",
    "    \n",
    "    save_directory = f'/home/isabella/Documents/isabella/jake/ephys_analysis/figures/P{obj.age}_{session}'\n",
    "    \n",
    "    # Load spikes for all good clusters\n",
    "    \n",
    "    autocorrelograms, first_moments = compute_autocorrelograms_and_first_moment(\n",
    "        spike_times = obj.spike_data['spike_times'],\n",
    "        spike_clusters = obj.spike_data['spike_clusters'],\n",
    "        bin_size = 0.001, #1ms\n",
    "        time_window = 0.05) #50ms\n",
    "        # burst_threshold = 0.01) #10 ms\n",
    "    \n",
    "    # Loop through each good cluster and plot autocorrelogram, labelled with burst index\n",
    "    for cluster, autocorrelogram in autocorrelograms.items():\n",
    "        fig, ax = plot_autocorrelogram(session, cluster, autocorrelograms[cluster], first_moments[cluster]) # burst_indices[cluster]\n",
    "        plt.savefig(f'{save_directory}/{cluster}_autocorrelogram.png')\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4195d2-706f-4cea-bfca-9c605aef1b80",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Plot mean waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c285a357-94af-4c42-8385-edff18c4359f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "from phylib.io.model import load_model\n",
    "import pandas as pd\n",
    "import os\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))\n",
    "from ephys import *\n",
    "\n",
    "# Load pickled data\n",
    "df_all_cells = pd.read_pickle('/home/isabella/Documents/isabella/jake/ephys_analysis/processed_data/df_all_cells.pkl')\n",
    "# Drop rows with no included clusters\n",
    "df_all_cells = df_all_cells.dropna()\n",
    "print(f'{len(df_all_cells.index)} sessions retained')\n",
    "\n",
    "data_path = '/home/isabella/Documents/isabella/jake/recording_data'\n",
    "\n",
    "# Loop through recording sessions\n",
    "for n, session in enumerate(df_all_cells.index):\n",
    "    \n",
    "    # Get path to params.py\n",
    "    obj = df_all_cells.loc[session, 'ephys_object']\n",
    "    \n",
    "    # Directory to save\n",
    "    save_directory = f'/home/isabella/Documents/isabella/jake/ephys_analysis/figures/P{obj.age}_{session}'\n",
    "    \n",
    "    # Load mean waveforms for all clusters\n",
    "    clusters = df_all_cells.loc[session, 'clusters_inc']\n",
    "    mean_waveforms = obj.load_mean_waveforms(clusters)\n",
    "    \n",
    "    for cluster in clusters:\n",
    "        \n",
    "        mean_waveform = mean_waveforms[cluster]\n",
    "        axes.plot(mean_waveform, c='black', alpha=1)\n",
    "        axes.set_title(f\"Session {session} (P{obj.age}) Cluster {cluster} Mean Waveform\")\n",
    "        axes.set_xlabel('Time (ms)')\n",
    "        axes.set_ylabel('Voltage (uV)')\n",
    "        # if np.min(mean_waveforms) > -200 and np.max(mean_waveforms) < 50:\n",
    "        #     axes.set_ylim(-200, 50)\n",
    "\n",
    "        ephys_sampling_rate = obj.spike_data['sampling_rate']\n",
    "        \n",
    "        # Calculate the time in milliseconds for each sample\n",
    "        time_ms = np.arange(len(mean_waveform)) / ephys_sampling_rate * 1000\n",
    "\n",
    "        # Set the x ticks to the calculated time in milliseconds\n",
    "        axes.set_xticks(np.arange(0, n_samples, int(0.5 * ephys_sampling_rate / 1000)))\n",
    "        axes.set_xticklabels(np.round(time_ms[::int(0.5 * ephys_sampling_rate / 1000)], 2))\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{save_directory}/{cluster}_waveform.png')\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ea11b-aa70-4253-8bf7-8d2c18528206",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Combine all plots for a single cluster into one image for checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e9fde4-2312-4978-96b8-0d76dc58559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "def combine_images_vertically(image_list):\n",
    "    \"\"\"\n",
    "    Combine a list of images vertically.\n",
    "    \n",
    "    Parameters:\n",
    "        image_list (list): List of Image objects to combine.\n",
    "        \n",
    "    Returns:\n",
    "        Image: Combined image.\n",
    "    \"\"\"\n",
    "    images = [Image.open(i) for i in image_list]\n",
    "    widths, heights = zip(*(i.size for i in images))\n",
    "    \n",
    "    total_height = sum(heights)\n",
    "    max_width = max(widths)\n",
    "    \n",
    "    new_img = Image.new('RGB', (max_width, total_height))\n",
    "    \n",
    "    y_offset = 0\n",
    "    x_offset = 700\n",
    "    for n, img in enumerate(images):\n",
    "        if 'waveform' in image_list[n]:\n",
    "            new_img.paste(img, (x_offset, 0))\n",
    "        else:\n",
    "            new_img.paste(img, (0, y_offset))\n",
    "            y_offset += img.height\n",
    "    \n",
    "    return new_img\n",
    "\n",
    "def clear_combined_images(subdir):\n",
    "    \"\"\"\n",
    "    Remove previously combined images in a given subdirectory.\n",
    "    \n",
    "    Parameters:\n",
    "        subdir (str): The path to the subdirectory.\n",
    "    \"\"\"\n",
    "    for file in os.listdir(subdir):\n",
    "        if file.endswith('_combined.png'):\n",
    "            os.remove(os.path.join(subdir, file))\n",
    "\n",
    "def process_directory(main_directory):\n",
    "    \"\"\"\n",
    "    Process the main directory to combine .png files with the same prefix within each subfolder.\n",
    "    \n",
    "    Parameters:\n",
    "        main_directory (str): The path to the main directory containing subfolders.\n",
    "    \"\"\"  \n",
    "    for subdir, _, files in os.walk(main_directory):\n",
    "        clear_combined_images(subdir)\n",
    "        \n",
    "        prefix_to_files = {}\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith('.png') and not file.endswith('_combined.png'):\n",
    "                prefix = f'{subdir}_{file[:3]}'\n",
    "                if prefix not in prefix_to_files:\n",
    "                    prefix_to_files[prefix] = []\n",
    "                prefix_to_files[prefix].append(os.path.join(subdir, file))\n",
    "                \n",
    "        for prefix, file_paths in prefix_to_files.items():\n",
    "            if len(file_paths) > 1:\n",
    "                file_paths.sort()\n",
    "                combined_img = combine_images_vertically(file_paths)\n",
    "                combined_img_path = os.path.join(subdir, f\"{prefix}_combined.png\")\n",
    "                combined_img.save(combined_img_path)\n",
    "                \n",
    "\n",
    "# Process all figures\n",
    "process_directory('/home/isabella/Documents/isabella/jake/ephys_analysis/figures')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84accc3f-ff1d-477a-a9e0-0266ee2e78d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Copy all combined images to a new directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7403455e-eb54-4498-b00c-109a26a5b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def copy_png_files(source_directory, target_directory, substring):\n",
    "    \"\"\"\n",
    "    Traverse through a directory and its subdirectories to find all .png files containing a specific substring.\n",
    "    Then copy these files to a target directory.\n",
    "\n",
    "    Parameters:\n",
    "    - source_directory (str): The directory path to start the traversal.\n",
    "    - target_directory (str): The directory where the files will be copied to.\n",
    "    - substring (str): The substring that the .png file names should contain.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure that the target directory exists; if not, create it.\n",
    "    if not os.path.exists(target_directory):\n",
    "        os.makedirs(target_directory)\n",
    "\n",
    "    # Traverse through the source directory and its subdirectories.\n",
    "    for dirpath, _, filenames in os.walk(source_directory):\n",
    "        for filename in filenames:\n",
    "            # Check if the file is a .png and contains the specific substring.\n",
    "            if filename.endswith('.png') and substring in filename:\n",
    "                source_file_path = os.path.join(dirpath, filename)\n",
    "                target_file_path = os.path.join(target_directory, filename)\n",
    "\n",
    "                # Copy the file to the target directory.\n",
    "                try:\n",
    "                    shutil.copy2(source_file_path, target_file_path)\n",
    "                except shutil.SameFileError:\n",
    "                    print(f\"File {filename} already exists in {target_directory}\")\n",
    "\n",
    "                # print(f\"Copied {filename} to {target_directory}\")\n",
    "\n",
    "copy_png_files(source_directory = '/home/isabella/Documents/isabella/jake/ephys_analysis/figures', target_directory = '/home/isabella/Documents/isabella/jake/ephys_analysis/figures/all_cells_combined_figures', substring = 'combined.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25abb12-1e18-4060-812d-6569d51f7bcc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Export names of all cell combined images to csv for manual curation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6c2111-8129-4c57-b0b2-05d0a6c70173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "# Specify the path to the directory containing the files\n",
    "folder_path = '/home/isabella/Documents/isabella/jake/ephys_analysis/figures/all_cells_combined_figures'\n",
    "\n",
    "# List all files and directories in the specified folder\n",
    "all_files_and_dirs = os.listdir(folder_path)\n",
    "\n",
    "# Filter out directories, keeping only files\n",
    "only_files = [f for f in all_files_and_dirs if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "# Remove file extensions from file names\n",
    "file_names_without_extension = [os.path.splitext(f)[0] for f in only_files]\n",
    "\n",
    "# Specify the path to the output CSV file\n",
    "csv_file_path = '/home/isabella/Documents/isabella/jake/ephys_analysis/figures/all_cells_combined_figures/list_of_figures.csv'\n",
    "\n",
    "# Write the list of file names to a CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    csv_writer = csv.writer(csvfile)\n",
    "    for file_name in file_names_without_extension:\n",
    "        csv_writer.writerow([file_name[:-9]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000ce61d-80ef-4c60-bea6-a9400a2eeeec",
   "metadata": {},
   "source": [
    "## Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
